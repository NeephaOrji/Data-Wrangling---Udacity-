{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Data Report by Jennifer Orji Chisom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello there, \n",
    "<br>\n",
    "<br>\n",
    "Today we are going to talk about all the processes I took to complete my data wrangling project.\n",
    "<br>\n",
    "## Data Gathering:\n",
    "<br>\n",
    "The first thing was to import the neccessary packages that i needed for the project. packages like numpy, matplotlib, pandas, os, json, tweepy, requests, re, warnings amongst others. Next, I gathered the data and this was done in three phases as the dataset were in 3 different files namely; Twitter archive data which I called tad as an abbreviation. Then the image prediction data set which I called url df and the tweet-json file provided by udacity as I could not get access to query twitter's API.\n",
    "<br>\n",
    "<br>\n",
    "You might be asking what I did with each dataset?  I will answer that in a bit.\n",
    "<br>\n",
    "<br>\n",
    "Let's look at tad dataset first. I checked the first and last 5 rows using basic pandas functions just to ensure that I copied the correct dataset.\n",
    "<br>\n",
    "<br>\n",
    "Next I used the requests library to download tweet image prediction by creating a directory, storing the url, opening the url file, reading it into a dataframe and assigning it to a variable called url_df. Then I used thesame basic Pandas Functions to check the first and last 5 rows of the dataset.\n",
    "<br>\n",
    "<br>\n",
    "Lastly in the gathering section of my wrangling project, I read the tweet-json file provided by udacity as I didn't have access to query twitter's API. This was extremely hard as my code kept coming back with a lot of errors that I could not  interpret, but thanks to the every ready community on slack, I was able to read the file successfuly by;\n",
    "<br>\n",
    "<br>\n",
    "* Creating an empty list\n",
    "<br>\n",
    "<br>\n",
    "* Opening and reading the text file\n",
    "<br>\n",
    "<br>\n",
    "* Appending it to a DataFrame and loaded it into a variable called df_tweets.\n",
    "<br>\n",
    "<br>\n",
    "Then I used Functions to see if it was done correctly and I felt like a magician."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asessing Data:\n",
    "<br>\n",
    "<br>\n",
    "The first thing I did in this section was to give a brief description of each column name to help me understand what each column in each dataset stood for. Next was to look at the 3 dataset visually to see what it looked like using Excel.\n",
    "<br>\n",
    "<br>\n",
    "Then I did a programatic assessment on each of the datasets using functions like .info(), .head(), .tail(), .sample(), .shape, .describe(), .duplicated() and .isnull(). All these helped me to understand each dataset more and Identify issues that needed cleaning. \n",
    "<br>\n",
    "<br>\n",
    "The first issue that caught my attention was missing data in some columns in tad. Some columns had as little as 78 non-null values out of 2356 entries. Thankfully, they were in columns that were not of so much importance to the questions my research posed.\n",
    "<br>\n",
    "Then I noticed some rows had figures over 10 in the rating denominator and most dog names started with lowercase. on further questioning, I discovered that those names that began with lower case were not even dog names to begin with, so I needed to address that.\n",
    "<br>\n",
    "Under the source column I noticed each source Value count had a hypertext reference which made it a tad bulky to read. I thought of easier ways to project the information in that column and documented that.\n",
    "<br>\n",
    "<br>\n",
    "I did thesame for the other two dataset(url_df and df_tweets). Then I grouped all the issues under quality and tidiness and the issues were 11 in total.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data:\n",
    "<br>\n",
    "<br>\n",
    "Here, I followed the 3 steps neccessary for successfull cleaning. The steps are Define, Code and Test. With these, I was able to tackle the following issues.\n",
    "<br>\n",
    "<br>\n",
    "* Tweet id in all dataset were integers instead of strings\n",
    "<br>\n",
    "<br>\n",
    "* Timestamps were strings instead of datetime datatype\n",
    "<br>\n",
    "<br>\n",
    "* Redundant data; the dog stages were in different columns instead of one\n",
    "<br>\n",
    "<br>\n",
    "* Missing values in columns like retweeted status timestamp, in reply to status id, in reply to user id, retweeted status id, retweeted status user id and expanded url\n",
    "<br>\n",
    "<br>\n",
    "* Irrelevant column; img_num in url_df\n",
    "<br>\n",
    "<br>\n",
    "* Incorrect dog names\n",
    "<br>\n",
    "<br>\n",
    "* Source column having a hypertext reference\n",
    "<br>\n",
    "<br>\n",
    "* Rating denominator not equal to 10 in some columns\n",
    "<br>\n",
    "<br>\n",
    "* Non null values in retweeted status id column\n",
    "<br>\n",
    "<br>\n",
    "* dog stages in different column instead of one column \n",
    "<br>\n",
    "<br>\n",
    "* Merging all 3 datasets on tweet id\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Dataset:\n",
    "<br>\n",
    "<br>\n",
    "Here, I saved the dataset into a dataset called twitter archive .csv and I moved to analysing and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing and Visualization:\n",
    "<br>\n",
    "<br>\n",
    "I read the cleaned and merged dataset as df and answered questions like\n",
    "<br>\n",
    "<br>\n",
    "* Which dog stage has the most retweets\n",
    "<br>\n",
    "<br>\n",
    "* Which dog stage has the most likes\n",
    "<br>\n",
    "<br>\n",
    "* How was the source tweet shared?\n",
    "<br>\n",
    "<br>\n",
    "* Is there a correlation between the retweets and likes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings me to the end of my wrandling project which was difficult at first, but I am more confident in my skills as a data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
